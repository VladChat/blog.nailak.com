# blog_src/scripts/rss_fetch.py
# -*- coding: utf-8 -*-
"""
ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· RSS Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.

ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹:
- Ğ Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ RSS-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (state["last_rss"])
- Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ state["seen"] (Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ URL)
- ĞŸÑ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ„Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸, Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´Ñ‘Ğ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚
- ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ»Ğ¾Ğ³: Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ğ»Ğ¸, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾
- ĞĞºĞºÑƒÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ state.json (Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° "seen")
"""

import json
import pathlib
import urllib.parse
import feedparser

STATE_PATH = pathlib.Path("blog_src/data/state.json")
RSS_PATH = pathlib.Path("blog_src/data/rss.json")
KEYWORDS_PATH = pathlib.Path("blog_src/data/keywords.json")

# ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµĞ¼Ñ‹Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ state.json Ğ½Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°ÑÑ‚Ğ°Ğ»ÑÑ)
SEEN_MAX = 500
# Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ„Ğ¸Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼
MAX_ENTRIES_PER_FEED = 15
# Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ñ€ÑĞ´ Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ·Ğ°Ğ¿ÑƒÑĞº (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ Ğ²ÑĞµÑ…)
MAX_FEEDS_TO_SCAN = 999999  # Ğ¿Ğ¾ ÑÑƒÑ‚Ğ¸ "Ğ²ÑĞµ", Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ» ĞºĞ°Ğº ÑĞ²Ğ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚


# === ğŸ“¦ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ JSON ===
def load_json(path: pathlib.Path, default):
    """Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ JSON Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ default (Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ¼)."""
    try:
        text = path.read_text(encoding="utf-8")
        data = json.loads(text)
        print(f"ğŸ“¥ Loaded JSON: {path} (size={len(text)} chars)")
        return data
    except Exception as e:
        print(f"âš ï¸ Failed to load JSON at {path}: {e}. Using default.")
        return default


def save_json(path: pathlib.Path, data):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ JSON Ñ Ğ¾Ñ‚ÑÑ‚ÑƒĞ¿Ğ°Ğ¼Ğ¸ Ğ¸ UTF-8."""
    try:
        path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
        print(f"ğŸ’¾ Saved JSON: {path}")
    except Exception as e:
        print(f"âŒ Failed to save JSON at {path}: {e}")


# === ğŸ”— ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ URL Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ===
def normalize_url(url: str) -> str:
    """
    Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ UTM/Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´Ñƒ.
    Ğ­Ñ‚Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹.
    """
    if not url:
        return url
    try:
        parsed = urllib.parse.urlsplit(url.strip())
        q = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)
        # Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
        filtered = [(k, v) for (k, v) in q if k.lower() not in {
            "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
            "gclid", "fbclid", "igshid"
        }]
        new_query = urllib.parse.urlencode(filtered, doseq=True)
        normalized = urllib.parse.urlunsplit((
            parsed.scheme.lower(),
            parsed.netloc.lower(),
            parsed.path.rstrip("/"),
            new_query,
            ""  # fragment ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼
        ))
        return normalized
    except Exception:
        return url.strip()


# === ğŸ§  ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ñ‹ ===
def get_latest_topic():
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ RSS-Ğ»ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· rss.json Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞµÑ‰Ñ‘ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚ÑŒÑ.
    ğŸ”¹ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ÑƒĞ¶Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ ÑÑÑ‹Ğ»ĞºĞ¸ (state["seen"])
    ğŸ”¹ Ğ Ğ¾Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ RSS-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸ keyword-Ğ¸Ğ½Ğ´ĞµĞºÑ
    ğŸ”¹ Ğ˜Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ Ñ„Ğ¸Ğ´Ğ°Ğ¼ Ğ¿Ğ¾ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸, Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´Ñ‘Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚: (topic_with_keyword, summary, original_url)
    """

    # 1) Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    rss_list = load_json(RSS_PATH, [])
    keywords = load_json(KEYWORDS_PATH, [])
    state = load_json(STATE_PATH, {"last_keyword": -1, "last_rss": -1, "seen": []})

    if not rss_list:
        raise RuntimeError("âš ï¸ rss.json is empty")
    if not keywords:
        raise RuntimeError("âš ï¸ keywords.json is empty")

    # 2) ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ
    last_rss = int(state.get("last_rss", -1))
    last_keyword = int(state.get("last_keyword", -1))
    seen_raw = state.get("seen", [])
    # Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ ÑƒĞ¶Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğµ URL
    seen = {normalize_url(u) for u in seen_raw if isinstance(u, str) and u.strip()}

    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("ğŸš€ RSS Picker starting")
    print(f"ğŸ“š RSS sources: {len(rss_list)} | ğŸ§  seen cache: {len(seen)} | ğŸ”‘ keywords: {len(keywords)}")
    print(f"ğŸ” Start rotation from RSS index: {last_rss + 1} (mod {len(rss_list)})")

    # 3) ĞŸĞµÑ€ĞµĞ±Ğ¾Ñ€ Ñ„Ğ¸Ğ´Ğ¾Ğ² Ñ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹
    feeds_scanned = 0
    selected = None
    selected_feed_index = None
    selected_feed_url = None

    while feeds_scanned < min(len(rss_list), MAX_FEEDS_TO_SCAN):
        rss_index = (last_rss + 1 + feeds_scanned) % len(rss_list)
        rss_url = rss_list[rss_index]
        feeds_scanned += 1

        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"ğŸŒ Checking RSS feed [{rss_index}]: {rss_url}")

        feed = feedparser.parse(rss_url)

        # Ğ›Ğ¾Ğ²Ğ¸Ğ¼ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ (bozo)
        if getattr(feed, "bozo", 0):
            print(f"âš ï¸ feedparser bozo=True for {rss_url}: {getattr(feed, 'bozo_exception', None)}")

        entries = getattr(feed, "entries", []) or []
        print(f"ğŸ“¦ Entries found: {len(entries)}")

        if not entries:
            print("â­ï¸ No entries in this feed. Moving on.")
            continue

        # 4) ĞŸĞµÑ€ĞµĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² ÑÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ´Ğµ
        checked = 0
        skipped = 0
        for entry in entries[:MAX_ENTRIES_PER_FEED]:
            checked += 1
            raw_link = (entry.get("link") or "").strip()
            if not raw_link:
                print("âšª Entry without link â€” skip")
                skipped += 1
                continue

            link = normalize_url(raw_link)
            if link in seen:
                print(f"ğŸ” Seen already: {raw_link}")
                skipped += 1
                continue

            # ĞĞ°ÑˆĞ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ
            selected = entry
            selected_feed_index = rss_index
            selected_feed_url = rss_url
            print(f"âœ… Selected NEW article after checking {checked} entries "
                  f"(skipped {skipped}) in feed [{rss_index}]")
            break

        if selected:
            break

        print(f"â­ï¸ No new articles in feed [{rss_index}] (checked {checked}, skipped {skipped}). Next feed...")

    # 5) Ğ•ÑĞ»Ğ¸ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ„Ğ¸Ğ´Ğ°Ñ… â€” Ğ°ĞºĞºÑƒÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ fallback
    if not selected:
        # Ğ’Ğ¾Ğ·ÑŒĞ¼Ñ‘Ğ¼ ÑĞ°Ğ¼ÑƒÑ ÑĞ²ĞµĞ¶ÑƒÑ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ´Ğ° Ğ¿Ğ¾ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ¾.
        fallback_index = (last_rss + 1) % len(rss_list)
        fallback_url = rss_list[fallback_index]
        feed = feedparser.parse(fallback_url)
        entries = getattr(feed, "entries", []) or []

        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("âš ï¸ No NEW articles across all feeds (all seen).")
        if not entries:
            raise RuntimeError("âŒ No entries available in fallback feed either â€” nothing to publish.")
        selected = entries[0]
        selected_feed_index = fallback_index
        selected_feed_url = fallback_url
        print(f"â™»ï¸ Reusing MOST RECENT article from feed [{fallback_index}]: {selected.get('link','')}")
        # Ğ’Ğ°Ğ¶Ğ½Ğ¾: Ğ² ÑÑ‚Ğ¾Ğ¼ fallback Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑÑÑ‹Ğ»ĞºÑƒ Ğ² seen, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Â«Ğ·Ğ°Ñ…Ğ»Ğ°Ğ¼Ğ»ÑÑ‚ÑŒÂ» Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğµ.

    # 6) Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    topic = (selected.get("title") or "Untitled").strip()
    summary = (selected.get("summary") or "").strip()
    orig_link = (selected.get("link") or "").strip()

    # 7) Ğ Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
    keyword_index = (last_keyword + 1) % len(keywords)
    keyword = keywords[keyword_index]

    # 8) ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼
    state["last_rss"] = selected_feed_index
    state["last_keyword"] = keyword_index

    # Ğ›Ğ¸Ğ½ĞºÑƒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² seen Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ (Ğ½Ğµ fallback-Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€)
    if orig_link and normalize_url(orig_link) not in seen:
        state_seen = state.get("seen", [])
        state_seen.append(orig_link)
        # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñƒ
        if len(state_seen) > SEEN_MAX:
            state_seen = state_seen[-SEEN_MAX:]
        state["seen"] = state_seen

    save_json(STATE_PATH, state)

    # 9) Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ»Ğ¾Ğ³ â€” ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ“° RSS Source: {selected_feed_url}")
    print(f"ğŸ§© Topic: {topic}")
    print(f"ğŸ“„ Summary: {summary[:250]}{'...' if len(summary) > 250 else ''}")
    print(f"ğŸ”— Link: {orig_link}")
    print(f"ğŸ¯ Using keyword: {keyword} (index {keyword_index})")
    print(f"ğŸ”„ last_rss -> {state['last_rss']} | ğŸ”‘ last_keyword -> {state['last_keyword']}")
    print(f"ğŸ—‚ seen size -> {len(state.get('seen', []))}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # 10) Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ Ğ² main.py
    return f"{topic} â€” {keyword}", summary, orig_link
