# blog_src/scripts/rss_fetch.py
# -*- coding: utf-8 -*-
"""
–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Ç–µ–º—ã –∏–∑ RSS —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:
- –†–æ—Ç–∞—Ü–∏—è RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (state["last_rss"])
- –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ state["seen"] (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π URL)
- –ü—Ä–æ—Å–º–æ—Ç—Ä –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∏–¥–æ–≤ –ø–æ –æ—á–µ—Ä–µ–¥–∏, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥—ë–º –Ω–æ–≤—ã–π –ø–æ—Å—Ç
- –ü–æ–¥—Ä–æ–±–Ω—ã–π –ª–æ–≥: —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏, —Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π, —á—Ç–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ, —á—Ç–æ –≤—ã–±—Ä–∞–Ω–æ
- –ê–∫–∫—É—Ä–∞—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ state.json (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ "seen")
"""

import json
import pathlib
import urllib.parse
import feedparser

STATE_PATH = pathlib.Path("blog_src/data/state.json")
RSS_PATH = pathlib.Path("blog_src/data/rss.json")
KEYWORDS_PATH = pathlib.Path("blog_src/data/keywords.json")

# –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–æ–º–∏–Ω–∞–µ–º—ã—Ö —Å—Å—ã–ª–æ–∫ (—á—Ç–æ–±—ã state.json –Ω–µ —Ä–∞–∑—Ä–∞—Å—Ç–∞–ª—Å—è)
SEEN_MAX = 500
# –°–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –≤ –∫–∞–∂–¥–æ–º —Ñ–∏–¥–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º—É–º
MAX_ENTRIES_PER_FEED = 15
# –°–∫–æ–ª—å–∫–æ —Ñ–∏–¥–æ–≤ –ø–æ–¥—Ä—è–¥ –ø—ã—Ç–∞–µ–º—Å—è –æ–±–æ–π—Ç–∏ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—É—Å–∫ (–æ–±—ã—á–Ω–æ —Ö–≤–∞—Ç–∏—Ç –≤—Å–µ—Ö)
MAX_FEEDS_TO_SCAN = 999999  # –ø–æ —Å—É—Ç–∏ "–≤—Å–µ", –æ—Å—Ç–∞–≤–∏–ª –∫–∞–∫ —è–≤–Ω—ã–π –ª–∏–º–∏—Ç


# === üì¶ –£—Ç–∏–ª–∏—Ç—ã —Ä–∞–±–æ—Ç—ã —Å JSON ===
def load_json(path: pathlib.Path, default):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç JSON –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç default (—Å –ª–æ–≥–æ–º)."""
    try:
        text = path.read_text(encoding="utf-8")
        data = json.loads(text)
        print(f"üì• Loaded JSON: {path} (size={len(text)} chars)")
        return data
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load JSON at {path}: {e}. Using default.")
        return default


def save_json(path: pathlib.Path, data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç JSON —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏ –∏ UTF-8."""
    try:
        path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
        print(f"üíæ Saved JSON: {path}")
    except Exception as e:
        print(f"‚ùå Failed to save JSON at {path}: {e}")


# === üîó –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è –ª—É—á—à–µ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ ===
def normalize_url(url: str) -> str:
    """
    –£–¥–∞–ª—è–µ–º UTM/—Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–º—É –≤–∏–¥—É.
    –≠—Ç–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –ª–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã.
    """
    if not url:
        return url
    try:
        parsed = urllib.parse.urlsplit(url.strip())
        q = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)
        # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        filtered = [(k, v) for (k, v) in q if k.lower() not in {
            "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
            "gclid", "fbclid", "igshid"
        }]
        new_query = urllib.parse.urlencode(filtered, doseq=True)
        normalized = urllib.parse.urlunsplit((
            parsed.scheme.lower(),
            parsed.netloc.lower(),
            parsed.path.rstrip("/"),
            new_query,
            ""  # fragment —É–±–∏—Ä–∞–µ–º
        ))
        return normalized
    except Exception:
        return url.strip()


# === üß† –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–π —Ç–µ–º—ã ===
def get_latest_topic():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS-–ª–µ–Ω—Ç—ã –∏–∑ rss.json –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—É—é –µ—â—ë –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é.
    üîπ –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –≤—Å—Ç—Ä–µ—á–∞–≤—à–∏–µ—Å—è —Å—Å—ã–ª–∫–∏ (state["seen"])
    üîπ –†–æ—Ç–∏—Ä—É–µ—Ç RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ keyword-–∏–Ω–¥–µ–∫—Å
    üîπ –ò–¥—ë—Ç –ø–æ —Ñ–∏–¥–∞–º –ø–æ –æ—á–µ—Ä–µ–¥–∏, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥—ë—Ç –Ω–æ–≤—ã–π –ø–æ—Å—Ç
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (topic_with_keyword, summary, original_url)
    """

    # 1) –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    rss_list = load_json(RSS_PATH, [])
    keywords = load_json(KEYWORDS_PATH, [])
    state = load_json(STATE_PATH, {"last_keyword": -1, "last_rss": -1, "seen": []})

    if not rss_list:
        raise RuntimeError("‚ö†Ô∏è rss.json is empty")
    if not keywords:
        raise RuntimeError("‚ö†Ô∏è keywords.json is empty")

    # 2) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    last_rss = int(state.get("last_rss", -1))
    last_keyword = int(state.get("last_keyword", -1))
    seen_raw = state.get("seen", [])
    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–∂–µ –≤–∏–¥–µ–Ω–Ω—ã–µ URL
    seen = {normalize_url(u) for u in seen_raw if isinstance(u, str) and u.strip()}

    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print("üöÄ RSS Picker starting")
    print(f"üìö RSS sources: {len(rss_list)} | üß† seen cache: {len(seen)} | üîë keywords: {len(keywords)}")
    print(f"üîÅ Start rotation from RSS index: {last_rss + 1} (mod {len(rss_list)})")

    # 3) –ü–µ—Ä–µ–±–æ—Ä —Ñ–∏–¥–æ–≤ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
    feeds_scanned = 0
    selected = None
    selected_feed_index = None
    selected_feed_url = None

    while feeds_scanned < min(len(rss_list), MAX_FEEDS_TO_SCAN):
        rss_index = (last_rss + 1 + feeds_scanned) % len(rss_list)
        rss_url = rss_list[rss_index]
        feeds_scanned += 1

        print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print(f"üåê Checking RSS feed [{rss_index}]: {rss_url}")

        feed = feedparser.parse(rss_url)

        # –õ–æ–≤–∏–º –ø–∞—Ä—Å–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (bozo)
        if getattr(feed, "bozo", 0):
            print(f"‚ö†Ô∏è feedparser bozo=True for {rss_url}: {getattr(feed, 'bozo_exception', None)}")

        entries = getattr(feed, "entries", []) or []
        print(f"üì¶ Entries found: {len(entries)}")

        if not entries:
            print("‚è≠Ô∏è No entries in this feed. Moving on.")
            continue

        # 4) –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å–∏ –≤ —ç—Ç–æ–º —Ñ–∏–¥–µ
        checked = 0
        skipped = 0
        for entry in entries[:MAX_ENTRIES_PER_FEED]:
            checked += 1
            raw_link = (entry.get("link") or "").strip()
            if not raw_link:
                print("‚ö™ Entry without link ‚Äî skip")
                skipped += 1
                continue

            link = normalize_url(raw_link)
            if link in seen:
                print(f"üîÅ Seen already: {raw_link}")
                skipped += 1
                continue

            # –ù–∞—à–ª–∏ –Ω–æ–≤—É—é
            selected = entry
            selected_feed_index = rss_index
            selected_feed_url = rss_url
            print(f"‚úÖ Selected NEW article after checking {checked} entries "
                  f"(skipped {skipped}) in feed [{rss_index}]")
            break

        if selected:
            break

        print(f"‚è≠Ô∏è No new articles in feed [{rss_index}] (checked {checked}, skipped {skipped}). Next feed...")

    # 5) –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ –≤–æ –≤—Å–µ—Ö —Ñ–∏–¥–∞—Ö ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π fallback
    if not selected:
        # –í–æ–∑—å–º—ë–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é –∑–∞–ø–∏—Å—å –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∏–¥–∞ –ø–æ —Ä–æ—Ç–∞—Ü–∏–∏, –Ω–æ —è–≤–Ω–æ –∑–∞–ª–æ–≥–∏—Ä—É–µ–º —ç—Ç–æ.
        fallback_index = (last_rss + 1) % len(rss_list)
        fallback_url = rss_list[fallback_index]
        feed = feedparser.parse(fallback_url)
        entries = getattr(feed, "entries", []) or []

        print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print("‚ö†Ô∏è No NEW articles across all feeds (all seen).")
        if not entries:
            raise RuntimeError("‚ùå No entries available in fallback feed either ‚Äî nothing to publish.")
        selected = entries[0]
        selected_feed_index = fallback_index
        selected_feed_url = fallback_url
        print(f"‚ôªÔ∏è Reusing MOST RECENT article from feed [{fallback_index}]: {selected.get('link','')}")
        # –í–∞–∂–Ω–æ: –≤ —ç—Ç–æ–º fallback –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ seen, —á—Ç–æ–±—ã –Ω–µ ¬´–∑–∞—Ö–ª–∞–º–ª—è—Ç—å¬ª –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–∏ –≤—ã–Ω—É–∂–¥–µ–Ω–Ω–æ–º –ø–æ–≤—Ç–æ—Ä–µ.

    # 6) –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    topic = (selected.get("title") or "Untitled").strip()
    summary = (selected.get("summary") or "").strip()
    orig_link = (selected.get("link") or "").strip()

    # 7) –†–æ—Ç–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    keyword_index = (last_keyword + 1) % len(keywords)
    keyword = keywords[keyword_index]

    # 8) –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    state["last_rss"] = selected_feed_index
    state["last_keyword"] = keyword_index

    # –õ–∏–Ω–∫—É –¥–æ–±–∞–≤–ª—è–µ–º –≤ seen —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–æ–≤–∞—è (–Ω–µ fallback-–ø–æ–≤—Ç–æ—Ä)
    if orig_link and normalize_url(orig_link) not in seen:
        state_seen = state.get("seen", [])
        state_seen.append(orig_link)
        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(state_seen) > SEEN_MAX:
            state_seen = state_seen[-SEEN_MAX:]
        state["seen"] = state_seen

    save_json(STATE_PATH, state)

    # 9) –ò—Ç–æ–≥–æ–≤—ã–π –ª–æ–≥ ‚Äî –∫—Ä–∞—Å–∏–≤–æ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print(f"üì∞ RSS Source: {selected_feed_url}")
    print(f"üß© Topic: {topic}")
    print(f"üìÑ Summary: {summary[:250]}{'...' if len(summary) > 250 else ''}")
    print(f"üîó Link: {orig_link}")
    print(f"üéØ Using keyword: {keyword} (index {keyword_index})")
    print(f"üîÑ last_rss -> {state['last_rss']} | üîë last_keyword -> {state['last_keyword']}")
    print(f"üóÇ seen size -> {len(state.get('seen', []))}")
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

    # 10) –í–æ–∑–≤—Ä–∞—Ç –≤ main.py
    return f"{topic} ‚Äî {keyword}", summary, orig_link
