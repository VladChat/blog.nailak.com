import json
import re
from datetime import datetime, timezone
from pathlib import Path

from . import llm
from . import posts
from .rss_fetch import get_latest_topic  # âœ… Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑƒĞ¶Ğµ ÑĞ²ĞµĞ¶ÑƒÑ, Ğ½ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚ÑŒÑ
from .config_loader import load_writer_config

# === ğŸ“‚ ĞŸÑƒÑ‚Ğ¸ Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ===
DATA_DIR = Path("blog_src/data")
KEYWORDS_FILE = DATA_DIR / "keywords.json"
STATE_FILE = DATA_DIR / "state.json"
CONTENT_DIR = Path("blog_src/content/posts")


# === ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° ===
def load_prompt_template() -> str:
    """Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°."""
    with open("blog_src/config/prompt_template.txt", "r", encoding="utf-8") as f:
        return f.read()


# === ğŸ”‘ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² ===
def load_keywords() -> list:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ keywords.json â€” Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ñ‚ĞµĞ¼ Ğ¸ SEO ĞºĞ»ÑÑ‡ĞµĞ¹."""
    with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


# === ğŸ’¾ Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ ===
def load_state() -> dict:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ state.json, ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ â€” ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ."""
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {"keyword_index": 0, "seen": []}


def save_state(state: dict) -> None:
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ state.json Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ°."""
    STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2)


# === ğŸ§© Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° ===
def build_prompt(topic: str, summary: str, original_url: str | None = None) -> str:
    """
    Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
    Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑÑÑ‹Ğ»ĞºÑƒ Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº, ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ.
    """
    template = load_prompt_template()
    topic_block = topic
    if original_url:
        topic_block += f"\n\nOriginal source: {original_url}"
    if summary:
        topic_block += f"\n\nContext: {summary}"
    else:
        topic_block += "\n\nContext: "
    return template.format(topic=topic_block)


# === ğŸ· ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞ³Ğ° ===
def _norm_tag(s: str) -> str:
    """ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ñ‚ĞµĞ³ (Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†Ğ°, Ğ´ĞµÑ„Ğ¸ÑÑ‹, Ğ±ĞµĞ· Ğ¼ÑƒÑĞ¾Ñ€Ğ°)."""
    s = (s or "").strip().lower()
    if not s:
        return ""
    out = []
    prev_dash = False
    for ch in s:
        if ch.isalnum():
            out.append(ch)
            prev_dash = False
        else:
            if not prev_dash:
                out.append("-")
                prev_dash = True
    t = "".join(out).strip("-")
    while "--" in t:
        t = t.replace("--", "-")
    return t[:40]


# === ğŸ§  Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»ÑÑ‡Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ ===
def _extract_secondary_from_article(md_text: str, all_keywords: list) -> str:
    """ĞŸÑ‹Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ (Ğ¿Ğ¾ keywords.json)."""
    if not md_text or not all_keywords:
        return ""
    text_low = md_text.lower()
    for kw in all_keywords:
        if kw.lower() in text_low:
            return _norm_tag(kw)
    return ""


# === ğŸ§  ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°: Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğ¸Ğ· Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° ===
def _extract_secondary_from_topic(topic: str, all_keywords: list) -> str:
    """Ğ•ÑĞ»Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ â€” Ğ¸Ñ‰ĞµĞ¼ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞµ."""
    if not topic or not all_keywords:
        return ""
    topic_low = topic.lower()
    for kw in all_keywords:
        if kw.lower() in topic_low:
            return _norm_tag(kw)
    return ""


# === ğŸ§¹ ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ„Ñ€Ğ°Ğ· Ğ´Ğ»Ñ meta keywords ===
def _clean_phrase_for_meta(s: str) -> str:
    """Ğ”ĞµĞ»Ğ°ĞµÑ‚ Ñ„Ñ€Ğ°Ğ·Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ´Ğ»Ñ meta keywords."""
    if not s:
        return ""
    s = re.sub(r"\s+", " ", str(s).strip())
    s = re.sub(r"^[,;|/]+", "", s)
    s = re.sub(r"[,;|/]+$", "", s)
    return s


# === ğŸš€ Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ===
def main():
    cfg = load_writer_config()
    state = load_state()

    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("ğŸš€ Starting Nailak writer")

    # === 1ï¸âƒ£ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² ===
    try:
        keywords = load_keywords()
        print(f"âœ… Loaded {len(keywords)} keywords")
    except Exception as e:
        print(f"âš ï¸ Could not load keywords.json: {e}")
        keywords = []

    idx = max(0, int(state.get("keyword_index", 0)))
    primary_keyword = keywords[idx] if keywords and idx < len(keywords) else ""
    print(f"ğŸ¯ Current primary keyword: {primary_keyword}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # === 2ï¸âƒ£ ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ rss_fetch ===
    print("ğŸ§­ Fetching RSS feed...")
    topic, summary, original_url = get_latest_topic()
    topic = topic or "Daily Nailak Update"
    summary = summary or ""
    original_url = original_url or None

    # ğŸ”¹ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ´ĞµĞ»Ğ°ĞµÑ‚ rss_fetch.py â€” Ğ·Ğ´ĞµÑÑŒ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾

    # === 3ï¸âƒ£ Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ===
    print("ğŸ“° Topic received:")
    print(f"Title: {topic}")
    print(f"Summary: {summary[:400]}{'...' if len(summary) > 400 else ''}")
    print(f"Original URL: {original_url if original_url else '(none)'}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # === 4ï¸âƒ£ Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ ===
    prompt = build_prompt(topic, summary, original_url)
    print("ğŸ§© Final topic-context sent to GPT:")
    print(prompt[:600] + ("..." if len(prompt) > 600 else ""))
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # === 5ï¸âƒ£ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ ===
    max_attempts = 3
    for attempt in range(max_attempts):
        print(f"ğŸ¤– Generating article (attempt {attempt + 1}/{max_attempts})...")
        md_raw = llm.call_llm(prompt)
        qa_result = posts.qa_check_proxy(md_raw)
        if qa_result["ok"]:
            print("âœ… QA passed.")
            break
        print(f"âš ï¸ QA failed: {qa_result['errors']}")
    else:
        print("âŒ All attempts failed â€” saving draft.")
        _save_draft(topic, cfg)
        return

    # === 6ï¸âƒ£ Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ³Ğ¾Ğ² ===
    secondary_tag = _extract_secondary_from_article(md_raw, keywords) or _extract_secondary_from_topic(topic, keywords)
    if not secondary_tag and keywords:
        secondary_tag = _norm_tag(keywords[(idx + 1) % len(keywords)])

    base_tags = []
    for i in range(2, 5):
        if len(keywords) > i:
            base_tags.append(_norm_tag(keywords[(idx + i) % len(keywords)]))

    keyword_tag = _norm_tag(primary_keyword)
    tags_list = []
    for t in [keyword_tag, secondary_tag, *base_tags]:
        if t and t not in tags_list:
            tags_list.append(t)
    if not tags_list:
        tags_list = ["nail-care"]

    tags_yaml = ", ".join("'" + t.replace("'", "''") + "'" for t in tags_list)

    # === 7ï¸âƒ£ Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ meta keywords ===
    primary_phrase = _clean_phrase_for_meta(primary_keyword)
    secondary_phrase = _clean_phrase_for_meta(secondary_tag.replace("-", " "))
    meta_keywords_parts = []
    if primary_phrase:
        meta_keywords_parts.append(primary_phrase)
    if secondary_phrase and secondary_phrase.lower() not in {p.lower() for p in meta_keywords_parts}:
        meta_keywords_parts.append(secondary_phrase)
    keywords_yaml_items = "".join([f'  - "{k}"\n' for k in meta_keywords_parts])
    keywords_block = f"keywords:\n{keywords_yaml_items}" if meta_keywords_parts else "keywords: []\n"

    # === 8ï¸âƒ£ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ğ¾ÑÑ‚ ===
    now = datetime.now(timezone.utc)
    slug_source = f"{topic} {primary_keyword}".strip()
    slug = posts.make_slug(slug_source)
    out_path = CONTENT_DIR / f"{now.year}/{now.month:02d}/{slug}.md"
    out_path.parent.mkdir(parents=True, exist_ok=True)

    title_escaped = topic.replace('"', '\\"')

    fm = (
        f"---\n"
        f'title: "{title_escaped}"\n'
        f"date: {now.isoformat()}\n"
        f"draft: false\n"
        f"categories: ['news']\n"
        f"tags: [{tags_yaml}]\n"
        f"{keywords_block}"
        f'author: "Nailak Editorial"\n'
        f"---\n\n"
    )

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(fm + md_raw)

    print("ğŸ§¾ Front-matter preview:")
    print(fm)
    print(f"âœ“ New post saved: {out_path}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    if keywords:
        state["keyword_index"] = (idx + 1) % len(keywords)
    save_state(state)
    print(f"ğŸ—‚ Updated state.json â€” next keyword index: {state['keyword_index']}")


# === ğŸ“ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾Ğµ ===
def _save_draft(topic: str, cfg: dict):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº, ĞµÑĞ»Ğ¸ QA Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑˆÑ‘Ğ» Ğ¸Ğ»Ğ¸ GPT Ğ½Ğµ Ğ´Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°."""
    now = datetime.now(timezone.utc)
    fallback_slug = re.sub(r"[^a-zA-Z0-9-]+", "-", topic.lower()) + "-draft"
    out_path = CONTENT_DIR / f"{now.year}/{now.month:02d}/{fallback_slug}.md"
    out_path.parent.mkdir(parents=True, exist_ok=True)

    title_escaped = topic.replace('"', '\\"')

    fm = (
        f"---\n"
        f'title: "{title_escaped}"\n'
        f"date: {now.isoformat()}\n"
        f"draft: true\n"
        f"categories: ['news']\n"
        f"tags: ['draft']\n"
        f'author: "Nailak Editorial"\n'
        f"---\n\n"
        f"(Auto-saved draft after QA failures)\n\n"
    )

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(fm)
    print(f"ğŸ“ Draft saved: {out_path}")


if __name__ == "__main__":
    main()
