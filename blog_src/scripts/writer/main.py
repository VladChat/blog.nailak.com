# blog_src/scripts/writer/main.py
import json
from datetime import datetime, timezone
from pathlib import Path

from . import llm
from . import posts
from .rss_fetch import get_latest_topic
from .config_loader import load_writer_config

DATA_DIR = Path("blog_src/data")
KEYWORDS_FILE = DATA_DIR / "keywords.json"
STATE_FILE = DATA_DIR / "state.json"
CONTENT_DIR = Path("blog_src/content/posts")


def load_prompt_template() -> str:
    with open("blog_src/config/prompt_template.txt", "r", encoding="utf-8") as f:
        return f.read()


def load_keywords() -> list:
    with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def load_state() -> dict:
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {"keyword_index": 0, "seen": []}


def save_state(state: dict) -> None:
    STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2)


def build_prompt(topic: str, summary: str, original_url: str | None = None) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞.
    –ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω URL –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ ‚Äî –≤—Å—Ç–∞–≤–ª—è–µ–º 'Original source: <url>' –≤ –±–ª–æ–∫ topic.
    """
    template = load_prompt_template()
    topic_block = topic
    if original_url:
        topic_block = f"{topic_block}\n\nOriginal source: {original_url}"
    if summary:
        topic_block = f"{topic_block}\n\nContext: {summary}"
    else:
        topic_block = f"{topic_block}\n\nContext: "
    return template.format(topic=topic_block)


def _norm_tag(s: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥ —Ç–µ–≥: –ª–∞—Ç–∏–Ω–∏—Ü–∞/—Ü–∏—Ñ—Ä—ã, –¥–µ—Ñ–∏—Å—ã –≤–º–µ—Å—Ç–æ –ø—Ä–æ—á–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤,
    —É—Ä–µ–∑–∞–Ω–∏–µ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö –¥–µ—Ñ–∏—Å–æ–≤, –æ–±—Ä–µ–∑–∫–∞ –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã.
    """
    s = (s or "").strip().lower()
    if not s:
        return ""
    out = []
    prev_dash = False
    for ch in s:
        if ch.isalnum():
            out.append(ch)
            prev_dash = False
        else:
            if not prev_dash:
                out.append("-")
                prev_dash = True
    t = "".join(out).strip("-")
    if not t:
        return ""
    while "--" in t:
        t = t.replace("--", "-")
    return t[:40]


def _extract_secondary_from_article(md_text: str) -> str:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å 1 –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ–≥ –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏.
    –ò—â–µ–º –ø–æ –±–µ–ª–æ–º—É —Å–ø–∏—Å–∫—É –¥–æ–º–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π, —á—Ç–æ–±—ã –Ω–µ —Ü–µ–ø–ª—è—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ —Å–ª–æ–≤–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô —Ç–µ–≥ (–¥–ª—è front-matter).
    """
    if not md_text:
        return ""
    candidates = [
        "glp-1", "ozempic", "semaglutide",
        "manicure", "nail polish", "cuticle oil",
        "strength training", "workout", "fitness", "wellness",
    ]
    text_low = md_text.lower()
    for c in candidates:
        if c in text_low:
            return _norm_tag(c)
    return ""


def _extract_secondary_from_topic(topic: str) -> str:
    """
    –§–æ–ª–ª–±—ç–∫: –ø—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å 1 —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ–≥ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ RSS/—Ç–æ–ø–∏–∫–∞,
    –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç–∞—Ç—å–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏.
    """
    if not topic:
        return ""
    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ + –Ω–µ–±–æ–ª—å—à–æ–π whitelisting
    tokens = [t for t in topic.lower().replace("‚Äî", " ").replace("-", " ").split() if len(t) > 2]
    white = {
        "glp", "glp1", "glp-1", "ozempic", "semaglutide",
        "manicure", "nail", "polish", "cuticle", "oil",
        "workout", "fitness", "wellness", "training", "strength",
    }
    for t in tokens:
        if t in white:
            return _norm_tag(t)
    # –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–∏—á–µ–≥–æ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–π ‚Äú—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π‚Äù —Ç–æ–∫–µ–Ω
    for t in tokens:
        if t.isalpha():
            return _norm_tag(t)
    return ""


def main():
    cfg = load_writer_config()

    # 1Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS (—Ç–µ–ø–µ—Ä—å —Ç—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏—è)
    topic, summary, original_url = get_latest_topic()
    topic = topic or "Travel update"
    summary = summary or ""
    original_url = original_url or None

    # üßæ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ topic-context
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print("üì∞ Extracted from RSS:")
    print(f"Title: {topic}")
    if summary:
        print(f"Summary: {summary[:400]}{'...' if len(summary) > 400 else ''}")
    else:
        print("Summary: (no summary provided)")
    print(f"Original source: {original_url if original_url else '(not detected)'}")
    print()

    topic_context_str = topic
    if original_url:
        topic_context_str += f"\n\nOriginal source: {original_url}"
    topic_context_str += f"\n\nContext: {summary}"

    print("üß© Final topic-context sent to GPT:")
    print(topic_context_str[:600] + ("..." if len(topic_context_str) > 600 else ""))
    print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

    # 2Ô∏è‚É£ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    try:
        keywords = load_keywords()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not load keywords.json: {e}")
        keywords = []

    state = load_state()
    idx = max(0, int(state.get("keyword_index", 0)))
    if keywords:
        if idx >= len(keywords):
            idx = 0
        keyword = (keywords[idx] or "").strip()
    else:
        keyword = ""

    # 3Ô∏è‚É£ –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    prompt = build_prompt(topic, summary, original_url)

    # 4Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å QA-–ø–æ–≤—Ç–æ—Ä–æ–º
    max_attempts = 3
    for attempt in range(max_attempts):
        md_raw = llm.call_llm(prompt)

        qa_result = posts.qa_check_proxy(md_raw)
        if qa_result["ok"]:
            slug_source = f"{topic} {keyword}".strip() if keyword else topic
            slug = posts.make_slug(slug_source)

            now = datetime.now(timezone.utc)
            out_path = CONTENT_DIR / f"{now.year}/{now.month:02d}/{slug}.md"
            out_path.parent.mkdir(parents=True, exist_ok=True)

            title = (topic or "Travel article").strip()
            title_escaped = title.replace('"', '\\"')

            default_category = cfg.get("default_category", "news")
            categories_json = f"['{default_category}']"

            # üîß –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –¢–ï–ì–û–í + SEO KEYWORDS (–†–ê–ó–î–ï–õ–ï–ù–ò–ï –†–û–õ–ï–ô)
            # 1) —è–∫–æ—Ä–Ω—ã–π SEO-–∫–ª—é—á (–∏–∑ keywords.json)
            keyword_tag = _norm_tag(keyword)

            # 2) –æ–¥–∏–Ω –∫–ª—é—á –∏–∑ –°–¢–ê–¢–¨–ò (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ) –∏–ª–∏ –∏–∑ TOPIC (—Ñ–æ–ª–ª–±—ç–∫)
            secondary_tag = _extract_secondary_from_article(md_raw) or _extract_secondary_from_topic(topic)

            # 3) —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ ‚Äú–∫–ª–µ—è—â–∏–µ‚Äù —Ç–µ–≥–∏ –¥–ª—è Related Posts
            base_tags = ["nail-care", "beauty-wellness"]

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤ (–±–µ–∑ –¥—É–±–ª–µ–π), –ø–æ—Ä—è–¥–æ–∫: —è–∫–æ—Ä—å ‚Üí –≤—Ç–æ—Ä–∏—á–Ω—ã–π ‚Üí –±–∞–∑–æ–≤—ã–µ
            tags_list = []
            for t in [keyword_tag, secondary_tag, *base_tags]:
                if t and (t not in tags_list):
                    tags_list.append(t)
            if not tags_list:
                tags_list = ["nail-care"]

            tags_yaml = ", ".join("'" + t.replace("'", "''") + "'" for t in tags_list)

            # SEO: meta keywords ‚Äî —Ç–æ–ª—å–∫–æ 1‚Äì2 –∑–Ω–∞—á–µ–Ω–∏—è (—á–∏—Å—Ç—ã–π —Ñ–æ–∫—É—Å)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—ã–π keyword + –≤—Ç–æ—Ä–∏—á–Ω—ã–π (–≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ñ—Ä–∞–∑—ã)
            secondary_phrase = secondary_tag.replace("-", " ").strip()
            meta_keywords_parts = []
            if keyword.strip():
                meta_keywords_parts.append(keyword.strip())
            if secondary_phrase and secondary_phrase not in meta_keywords_parts:
                meta_keywords_parts.append(secondary_phrase)
            meta_keywords = ", ".join(meta_keywords_parts)

            fm = (
                f"---\n"
                f'title: "{title_escaped}"\n'
                f"date: {now.isoformat()}\n"  # ‚úÖ —É–±—Ä–∞–Ω –ª–∏—à–Ω–∏–π 'Z'
                f"draft: false\n"
                f"categories: {categories_json}\n"
                f"tags: [{tags_yaml}]\n"
                f'keywords: "{meta_keywords}"\n'
                f'author: "Nailak Editorial"\n'
                f"---\n\n"
            )

            print("üßæ Front-matter preview:")
            print(fm)
            print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

            with open(out_path, "w", encoding="utf-8") as f:
                f.write(fm + md_raw)

            print(f"‚úì New post: {out_path}")

            if keywords:
                next_idx = (idx + 1) % len(keywords)
                state["keyword_index"] = next_idx
                save_state(state)

            return
        else:
            print(f"‚ö†Ô∏è Attempt {attempt + 1} failed QA: {qa_result['errors']}")

    # 5Ô∏è‚É£ –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî —á–µ—Ä–Ω–æ–≤–∏–∫
    if cfg.get("draft_if_fail", True):
        now = datetime.now(timezone.utc)
        fallback_slug = posts.make_slug(f"{topic}-draft")
        out_path = CONTENT_DIR / f"{now.year}/{now.month:02d}/{fallback_slug}.md"
        out_path.parent.mkdir(parents=True, exist_ok=True)

        title = (topic or "Travel article").strip()
        title_escaped = title.replace('"', '\\"')
        default_category = cfg.get("default_category", "news")
        categories_json = f"['{default_category}']"

        fm = (
            f"---\n"
            f'title: "{title_escaped}"\n'
            f"date: {now.isoformat()}\n"  # ‚úÖ —É–±—Ä–∞–Ω –ª–∏—à–Ω–∏–π 'Z'
            f"draft: true\n"
            f"categories: {categories_json}\n"
            f"tags: ['draft']\n"
            f'author: "Nailak Editorial"\n'
            f"---\n\n"
            f"(Auto-saved draft after QA failures)\n\n"
        )

        with open(out_path, "w", encoding="utf-8") as f:
            f.write(fm)

        print(f"üìù Saved draft: {out_path}")
    else:
        print("‚ùå Failed to generate a valid post after retries.")


if __name__ == "__main__":
    main()
